# train_ppo.py
"""
ä½¿ç”¨æ–°å¥–åŠ±å‡½æ•°è®­ç»ƒ PPO æ¨¡å‹
"""

import os
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
from carla_env.carla_env_multi_obs import CarlaEnvMultiObs

if __name__ == "__main__":
    # åˆ›å»ºç¯å¢ƒ
    env = CarlaEnvMultiObs(
        random_spawn=True,
        max_episode_steps=1000,
        debug=False
    )

    # æ—¥å¿—å’Œæ¨¡å‹ä¿å­˜è·¯å¾„
    log_dir = "./logs/"
    checkpoint_dir = "./checkpoints/"
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(checkpoint_dir, exist_ok=True)

    # å›è°ƒï¼šæ¯ 10k æ­¥ä¿å­˜ä¸€æ¬¡
    checkpoint_callback = CheckpointCallback(
        save_freq=10000,
        save_path=checkpoint_dir,
        name_prefix="ppo_carla"
    )

    # åˆ›å»º PPO æ¨¡å‹
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        tensorboard_log=log_dir,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01
    )

    # å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ PPO æ¨¡å‹...")
    model.learn(
        total_timesteps=500_000,
        callback=checkpoint_callback,
        tb_log_name="ppo_run"
    )

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save(os.path.join(checkpoint_dir, "best_model.zip"))
    print("âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³ ./checkpoints/best_model.zip")
    env.close()