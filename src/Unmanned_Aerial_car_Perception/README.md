# 无人车多传感器融合感知技术研究与工程实现

## 1 引言

### 1.1 研究背景与意义

自动驾驶技术是智能交通体系的核心组成部分，其发展可显著提升交通效率、降低交通事故率，推动交通运输行业的智能化转型[1]。感知系统作为无人车的“感官中枢”，承担着环境认知与自身状态估计的关键任务，其性能直接决定自动驾驶的安全性与可靠性[2]。无人车感知需同步完成两大核心目标：一是环境感知，即实时、精准地识别道路边界、车道线、障碍物（车辆、行人、非机动车）、交通标识等外部环境信息；二是状态感知，即精确估计车辆自身的位置、速度、航向角、姿态等内部状态参数[3]。
当前，单传感器感知方案在实际应用中存在诸多痛点：视觉感知依赖光照条件，在强光、逆光、雨雾等恶劣天气下鲁棒性显著下降；LiDAR虽具备高精度三维测距能力，但成本高昂，点云数据存在稀疏性问题，且在远距离场景下性能衰减明显；IMU可提供高频姿态数据，但存在积分累积误差；GPS/RTK能提供绝对位置信息，但更新频率低，在隧道、高楼密集区等遮挡场景下易失效[4]。此外，多传感器数据存在时空异步性，融合算法复杂度高，如何在嵌入式平台有限算力下实现高精度与低延迟的平衡，是无人车感知技术落地的核心挑战。因此，研究低成本、高鲁棒、低延迟的多传感器融合感知技术，对推动L2-L4级自动驾驶的规模化应用具有重要的工程价值与学术意义。

### 1.2 国内外研究现状

### 1.2.1 单传感器感知技术

视觉感知因成本低、信息丰富的优势，成为无人车感知的基础模块。早期研究多采用传统计算机视觉方法，如基于霍夫变换的车道线检测 [5]、基于背景差分的障碍物检测 [6]，但此类方法泛化能力弱，难以适应复杂多变的道路场景。
近年来，深度学习技术推动视觉感知精度的大幅提升，YOLO 系列、Faster R-CNN 等 2D 目标检测模型已广泛应用于障碍物识别 [7-8]；随着 BEV（Bird's Eye View）技术的发展，BEVFormer、DETR3D 等 3D 视觉模型实现了无 LiDAR 的三维感知，但其性能依赖大规模标注数据集与强大的算力支撑 [9-10]。LiDAR 凭借不受光照影响、三维测距精度高的特性，成为高阶自动驾驶的核心传感器。
主流 LiDAR 感知技术包括：
* 基于 RANSAC 算法的地面分割 [11]，可快速分离地面与非地面点云；
* 基于 DBSCAN、欧式聚类的障碍物检测 [12]，能有效提取离散点云中的障碍物轮廓；
* 基于 PointNet 系列的点云深度学习模型 [13]，实现了障碍物的分类与语义分割。

但 LiDAR 硬件成本较高，点云数据存在稀疏性问题，在远距离、雨雾雪等恶劣天气下感知性能易受影响，且现有研究多聚焦于障碍物检测精度，缺乏对 LiDAR 点云降噪、障碍物置信度校验的工程化优化，也未实现感知结果与车辆速度控制的联动。惯性与卫星导航感知是无人车状态估计的核心手段。IMU（惯性测量单元）更新频率可达 100Hz 以上，能实时输出姿态与加速度数据，但长期使用会产生累积误差；GPS/RTK 可提供绝对位置信息，定位精度可达厘米级，但更新频率仅 10Hz 左右，易受遮挡影响 [14]。扩展卡尔曼滤波（EKF）、无迹卡尔曼滤波（UKF）是 IMU 与 GPS 融合的经典方法 [15]，通过互补滤波降低单一传感器误差；紧耦合 SLAM 技术（如 RTAB-Map [16]）进一步融合视觉信息，提升了无 GPS 场景下的定位鲁棒性，但现有状态估计未充分结合车辆速度闭环控制需求，难以实现精准匀速行驶。



### 1.2.2 多传感器融合技术

多传感器融合是解决单传感器缺陷的核心路径，根据融合层次可分为数据级、特征级与决策级融合三类[17]。数据级融合直接处理原始传感器数据，如视觉图像与LiDAR点云的像素级配准[18]，精度最高但计算复杂度大；特征级融合先提取各传感器的特征信息（如视觉SIFT特征、点云FPFH特征），再进行特征拼接与融合[19]，兼顾精度与效率；决策级融合对各传感器的检测结果进行加权投票或概率融合[20]，复杂度最低但精度依赖单传感器性能。
当前主流融合算法可分为三类：基于规则的融合（如加权平均法）、基于滤波的融合（EKF/UKF）与基于深度学习的融合（如FusionNet[21]、PointPillars[22]）。其中，EKF因计算效率高、适应性强，被广泛应用于无人车状态估计与多传感器融合；深度学习融合方法通过神经网络自动学习多源数据的关联特征，融合精度更高，但对算力要求严格，难以在嵌入式平台实时运行[23]。

### 1.2.3 嵌入式感知系统

无人车感知系统需部署于嵌入式平台（如NVIDIA Jetson系列、Mobileye EyeQ系列），需在算力受限条件下实现高精度与低延迟的平衡[24]。为提升嵌入式平台的感知实时性，研究人员主要从两个方向突破：一是模型轻量化，通过量化、剪枝、蒸馏等技术压缩模型参数量[25]，如YOLOv8n轻量化模型的参数量仅为6.2M，较传统模型推理速度提升3倍以上；二是硬件加速，利用CUDA、TensorRT等工具对模型进行推理加速[26]，将深度学习模型的推理延迟降至毫秒级。此外，并行计算技术（如多线程、多进程）的应用，可实现视觉、LiDAR、融合等模块的异步并行处理，进一步提升系统整体运行效率[27]。

### 1.3 本文主要研究内容与创新点

本文围绕无人车多传感器融合感知技术展开研究，核心内容与创新点如下
* 1.提出一种“视觉+LiDAR+IMU+GPS”的多模态感知框架，通过多传感器互补优势提升复杂场景下的感知鲁棒性，同时采用低成本传感器组合（16线LiDAR+普通工业相机）降低系统部署成本；
* 2.设计改进EKF融合算法，引入车道线偏移、障碍物距离等视觉/LiDAR特征作为观测值，修正IMU累积误差与GPS定位偏差，提升状态估计精度；
* 3.基于Jetson Xavier嵌入式平台搭建实时感知系统，通过模型轻量化（YOLOv8n量化）、硬件加速（TensorRT）与并行计算优化，实现感知端到端延迟≤30ms；
* 4.在园区、城区、乡村三类典型场景下开展大量实验，量化分析系统的感知精度、延迟与鲁棒性，验证方案的工程可行性。

### 1.4 论文结构安排

本文共分为5章：第1章为引言，阐述研究背景、国内外研究现状、研究内容与创新点；第2章为感知方法与系统设计，详细介绍视觉感知、LiDAR感知、多传感器融合算法及嵌入式系统实现方案；第3章为实验设计与结果分析，通过多场景实验验证系统性能；第4章为讨论与展望，分析现有研究的不足并提出未来改进方向；第5章为结论，总结全文研究成果。

## 2 感知方法与系统设计

### 2.1 系统整体架构

本文设计的无人车多传感器融合感知系统采用分层架构，分为感知层、融合层与输出层，整体架构如图1所示。
各层功能如下：
* •感知层：由1080P@30fps工业相机、16线机械式LiDAR、100Hz IMU、10Hz GPS/RTK及50Hz轮速计组成，完成原始数据采集与单传感器初步处理（如图像预处理、点云去噪）；
* •融合层：核心层，负责多传感器数据的时间同步、空间配准，通过改进EKF算法完成多源特征融合与误差修正，输出统一的感知结果；
* •输出层：对融合后的感知结果进行标准化处理，生成障碍物信息（位置、类别、距离）、车道线信息（偏移量、曲率）、自身状态信息（位置、速度、航向角），供决策控制模块调用。

### 2.2 视觉感知模块设计

视觉感知模块的核心任务是车道线检测与障碍物检测，采用“传统视觉+深度学习”的混合方案，兼顾检测精度与实时性。

### 2.2.1 车道线检测

车道线检测采用“预处理-候选提取-深度学习修正”的三步法，具体流程如下：
* 图像预处理：首先对相机采集的彩色图像进行灰度化处理，减少计算量；采用5×5高斯滤波器进行平滑去噪，降低图像噪声对边缘检测的影响；通过自适应阈值二值化将灰度图像转换为二值图像，增强车道线与背景的对比度；最后采用Canny边缘检测算法提取图像边缘特征。
* 感兴趣区域（ROI）裁剪：由于车道线仅分布在图像下半部分的道路区域，通过裁剪ROI（通常为图像底部1/2区域）可有效减少背景干扰，提升计算效率。
* 候选车道线提取：采用概率霍夫变换（HoughLinesP）提取ROI内的直线候选，设置直线最小长度50px、最大间隙100px，过滤短距离噪声直线；根据直线斜率分离左、右车道线候选（左车道线斜率为负，右车道线斜率为正）。
* 深度学习修正：采用轻量化MobileNetV2作为骨干网络，对车道线候选进行分类筛选，去除非车道线噪声；通过多项式拟合得到左、右车道线的连续曲线，计算车道线中心与车辆中心的偏移量。
车道线偏移量计算公式如式（1）所示：
$$d = (x_c - x_f) \times k \tag{1}$$
其中，$x_c$为车道线中心的像素坐标，$x_f$为车辆中心对应的像素坐标，$k$为像素-米转换系数（通过相机标定获得，本文中$k=0.008$m/像素），$d$为车道线偏移量（正值表示车辆偏右，负值表示车辆偏左）。

### 2.2.2 障碍物检测

采用轻量化YOLOv8n模型实现视觉障碍物检测，针对无人车场景进行以下优化：
$$D = \frac{H \times f}{h} \tag{2}$$
其中，$H$为目标实际高度（如轿车1.5m、行人1.7m），$f$为相机焦距（像素，通过相机标定获得），$h$为检测框高度（像素），$D$为障碍物与车辆的距离（米）。

### 2.3 LiDAR感知模块设计

LiDAR感知模块的核心任务是点云预处理与障碍物检测，通过地面分割与聚类算法提取障碍物的三维信息。

### 2.3.1 点云预处理

LiDAR原始点云包含大量噪声与冗余数据，需进行预处理以提升后续检测精度，具体步骤如下：
离群点去除：采用统计滤波算法，设置邻域点数20、标准差阈值2.0，剔除距离邻域点过远的噪声点；
点云下采样：采用体素下采样算法，设置体素大小0.2m×0.2m×0.2m，减少点云数量，提升计算效率；
地面分割：采用RANSAC算法拟合地面平面（平面模型为$ax+by+cz+d=0$），设置迭代次数1000、距离阈值0.1m，分离地面点云与非地面点云（障碍物点云）。
2.3.2 障碍物聚类与特征提取
* 对非地面点云采用DBSCAN算法进行聚类，提取障碍物的三维特征，具体步骤如下：
* 聚类参数设置：邻域半径$\epsilon=0.5$m，最小聚类点数$min\_points=10$，确保能有效聚类车辆、行人等不同尺寸的障碍物；
* 特征计算：对每个聚类簇，计算其中心坐标$(x,y,z)$、与车辆的距离$D=\sqrt{x^2+y^2+z^2}$、方位角$\theta=\arctan2(y,x)$，以及障碍物的长、宽、高（聚类簇在x、y、z轴方向的最大差值）。

### 2.4 多传感器融合算法

多传感器融合的核心是解决数据的时空配准与误差修正问题，本文采用改进EKF算法实现多源数据融合。

### 2.4.1 时空配准

时间同步：以IMU数据为基准（更新频率100Hz），对视觉数据（30Hz）与LiDAR数据（10Hz）进行线性插值，将所有传感器数据统一到同一时间戳下，消除时间异步性；
空间配准：通过手眼标定实验获取相机与LiDAR的外参矩阵$T_{cam-lidar}$（含旋转矩阵$R$与平移向量$t$），将视觉图像中的像素坐标转换到LiDAR坐标系下，实现视觉特征与点云特征的空间对齐。外参矩阵转换公式如式（3）所示：
$$P_{lidar} = R \times P_{cam} + t \tag{3}$$
其中，$P_{cam}$为相机坐标系下的特征点坐标，$P_{lidar}$为转换后的LiDAR坐标系下坐标。

### 2.4.2 改进EKF融合算法

传统EKF仅融合IMU与GPS数据，本文引入视觉/LiDAR特征作为观测值，优化状态方程与观测方程，提升融合精度。

### 2.4.2.1 状态方程

定义状态向量$\mathbf{X}=[x,y,v_x,v_y,\theta]^T$，其中$x,y$为车辆在大地坐标系下的位置，$v_x,v_y$为车辆在x、y方向的速度，$\theta$为航向角。状态方程如式（4）所示：
$$\mathbf{X}_{k} = \mathbf{F}_k \mathbf{X}_{k-1} + \mathbf{G}_k \mathbf{w}_{k-1} \tag{4}$$
其中，$\mathbf{F}_k$为状态转移矩阵，$\mathbf{G}_k$为过程噪声驱动矩阵，$\mathbf{w}_{k-1}$为过程噪声（服从高斯分布$N(0,\mathbf{Q}_k)$）。状态转移矩阵$\mathbf{F}_k$如式（5）所示：
$$\mathbf{F}_k = \begin{bmatrix} 1 & 0 & \Delta t & 0 & 0 \\ 0 & 1 & 0 & \Delta t & 0 \\ 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1 \end{bmatrix} \tag{5}$$
其中，$\Delta t$为相邻时刻的时间间隔。

### 2.4.2.2 观测方程

定义观测向量$\mathbf{Z}=[x_{gps},y_{gps},d_{lane},D_{lidar}]^T$，其中$x_{gps},y_{gps}$为GPS提供的位置信息，$d_{lane}$为视觉检测的车道线偏移量，$D_{lidar}$为LiDAR检测的障碍物距离。观测方程如式（6）所示：
$$\mathbf{Z}_k = \mathbf{H}_k \mathbf{X}_k + \mathbf{v}_k \tag{6}$$
其中，$\mathbf{H}_k$为观测矩阵，$\mathbf{v}_k$为观测噪声（服从高斯分布$N(0,\mathbf{R}_k)$）。观测矩阵$\mathbf{H}_k$如式（7）所示：
$$\mathbf{H}_k = \begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & k_d \\ 0 & 0 & 0 & 0 & k_D \end{bmatrix} \tag{7}$$
其中，$k_d,k_D$为车道线偏移量、障碍物距离与状态向量的关联系数，通过实验标定获得。

### 2.4.2.3 滤波更新流程

EKF融合的核心流程包括预测与更新两个阶段：
预测阶段：根据上一时刻的状态估计值与状态转移矩阵，预测当前时刻的先验状态与先验协方差矩阵：
        $$\hat{\mathbf{X}}_k^- = \mathbf{F}_k \hat{\mathbf{X}}_{k-1} \tag{8}$$
        $$\mathbf{P}_k^- = \mathbf{F}_k \mathbf{P}_{k-1} \mathbf{F}_k^T + \mathbf{Q}_k \tag{9}$$
        其中，$\hat{\mathbf{X}}_k^-$为当前时刻先验状态估计值，$\mathbf{P}_k^-$为先验协方差矩阵，$\mathbf{P}_{k-1}$为上一时刻后验协方差矩阵，$\mathbf{Q}_k$为过程噪声协方差矩阵。
更新阶段：根据观测向量与先验状态，计算卡尔曼增益，更新后验状态与后验协方差矩阵：
        $$\mathbf{K}_k = \mathbf{P}_k^- \mathbf{H}_k^T (\mathbf{H}_k \mathbf{P}_k^- \mathbf{H}_k^T + \mathbf{R}_k)^{-1} \tag{10}$$
        $$\hat{\mathbf{X}}_k = \hat{\mathbf{X}}_k^- + \mathbf{K}_k (\mathbf{Z}_k - \mathbf{H}_k \hat{\mathbf{X}}_k^-) \tag{11}$$
        $$\mathbf{P}_k = (\mathbf{I} - \mathbf{K}_k \mathbf{H}_k) \mathbf{P}_k^- \tag{12}$$
        其中，$\mathbf{K}_k$为卡尔曼增益，$\mathbf{R}_k$为观测噪声协方差矩阵，$\mathbf{I}$为单位矩阵，$\hat{\mathbf{X}}_k$为当前时刻后验状态估计值，$\mathbf{P}_k$为后验协方差矩阵。
本文中，过程噪声协方差矩阵$\mathbf{Q}_k$与观测噪声协方差矩阵$\mathbf{R}_k$通过实验标定获得，具体取值为：
    $$\mathbf{Q}_k = \text{diag}(0.01,0.01,0.001,0.001,0.005) \tag{13}$$
    $$\mathbf{R}_k = \text{diag}(0.1,0.1,0.05,0.02) \tag{14}$$

### 2.5 嵌入式系统实现

基于NVIDIA Jetson Xavier NX嵌入式平台搭建感知系统，硬件配置为8核ARM Cortex-A78CPU、16GB LPDDR4X内存、384核CUDA GPU；软件环境为Ubuntu 20.04 LTS、ROS Noetic、CUDA 11.4、TensorRT 8.5。
系统优化策略如下：
* 并行计算优化：采用多线程技术，将视觉感知、LiDAR感知、融合模块部署为独立线程，通过共享内存实现数据交互，提升系统整体运行效率；
* 模型加速：利用TensorRT对YOLOv8n模型进行推理加速，通过量化、层融合等优化手段，将模型推理时间从20ms降至5ms；
* 内存优化：采用循环缓冲区存储图像与点云数据，避免内存泄漏；对大尺寸数据采用内存映射机制，减少数据拷贝时间；
* 驱动优化：优化传感器驱动程序，提升数据采集效率，降低数据传输延迟。

## 3 实验与结果分析

### 3.1 实验平台与数据集

### 3.1.1 硬件平台

实验采用改装无人车平台，核心硬件配置如表1所示：
表1 无人车感知系统硬件配置

| 模块       | 型号                  | 关键参数                          |
|------------|-----------------------|-----------------------------------|
| 车载计算平台 | NVIDIA Jetson Xavier NX | 8核CPU,384核GPU,16GB内存          |
| 相机       | IMX219工业相机        | 1080P/30fps,焦距6mm,视场角70°    |
| LiDAR      | Velodyne VLP-16       | 16线,0.1-100m,精度±3cm           |
| IMU        | MPU6050               | 100Hz,姿态精度±0.1°               |
| GPS/RTK    | UBLOX M8N             | 10Hz,RTK精度±0.5m                 |
| 轮速计     | 霍尔传感器            | 50Hz,测速精度±0.1m/s              |

### 3.1.2 实验场景与数据集

实验选取三类典型自动驾驶场景，覆盖不同路况与环境条件：
* 园区场景：低速行驶（≤10km/h），包含行人、非机动车密集区域，无交通信号灯；
* 城区场景：中速行驶（≤30km/h），包含多车道、交叉路口、交通信号灯，车辆密集；
* 乡村场景：中高速行驶（≤40km/h），道路狭窄，无清晰车道线，存在树木、建筑物遮挡。
实验数据集包括公开数据集与自制数据集：公开数据集采用KITTI[28]、BDD100K[29]，涵盖城区、乡村等场景；自制数据集通过实验平台在园区场景采集，含10000帧图像、点云数据，人工标注障碍物、车道线等真值信息。测试集为三类场景各1000帧数据，确保测试数据的多样性与代表性。

### 3.2 评价指标

为全面评价感知系统性能，选取以下评价指标：
* 障碍物检测精度：采用平均精度均值（mAP）、精确率（Precision）、召回率（Recall）评价，其中mAP为各类别目标平均精度的均值，是目标检测的核心评价指标；
* 车道线检测精度：采用偏移估计误差（MAE、RMSE）评价，MAE为平均绝对误差，RMSE为均方根误差，误差越小表示检测精度越高；
* 状态估计精度：采用位置误差（MAE）、航向角误差（MAE）评价；
* 实时性：采用端到端感知延迟（从传感器数据采集到感知结果输出的总时间）评价。

### 3.3 实验结果与分析

### 3.3.1 障碍物检测结果

将本文提出的融合感知方案与单视觉（YOLOv8n）、单LiDAR（DBSCAN聚类）方案进行对比，障碍物检测性能如表2所示：
表2 不同方案障碍物检测性能对比

| 方案         | 精确率（%） | 召回率（%） | mAP（%） |
|--------------|-------------|-------------|----------|
| 单视觉（YOLOv8n） | 88.5        | 86.2        | 87.3     |
| 单LiDAR（DBSCAN） | 90.3        | 89.1        | 89.7     |
| 本文融合方案   | 95.2        | 94.1        | 94.7     |
由表2可知，本文融合方案的精确率、召回率与mAP均显著高于单传感器方案，mAP较单视觉方案提升7.4个百分点，较单LiDAR方案提升5个百分点。这是因为融合方案结合了视觉的类别识别优势与LiDAR的高精度测距优势，通过互补性提升了障碍物检测的鲁棒性。在雨雾、逆光等恶劣环境下，单视觉方案mAP下降至72.5%，单LiDAR方案mAP下降至82.3%，而本文融合方案mAP仍保持在89.6%，验证了融合方案的环境适应性。

### 3.3.2 车道线检测结果

车道线偏移估计误差对比结果如表3所示：
表3 车道线偏移估计误差对比

| 方案         | MAE（m） | RMSE（m） |
|--------------|----------|-----------|
| 传统霍夫变换 | 0.18     | 0.22      |
| 本文混合方案 | 0.07     | 0.09      |

### 3.3.3 状态估计结果

状态估计精度对比结果如表4所示：

| 方案                | 位置误差MAE（m） | 航向角误差MAE（°） |
|---------------------|------------------|--------------------|
| GPS+IMU（传统EKF）  | 0.32             | 0.85               |
| 本文改进EKF         | 0.15             | 0.32               |
由表4可知，本文改进EKF方案的位置误差MAE为0.15m，航向角误差MAE为0.32°，较传统EKF方案分别降低53.1%、62.4%。这是因为改进EKF引入了车道线偏移、障碍物距离等视觉/LiDAR特征作为观测值，有效修正了IMU的累积误差与GPS的定位偏差，尤其在GPS信号丢失的隧道场景下，传统EKF方案位置误差迅速增大至1.2m，而本文改进方案位置误差仍控制在0.3m以内，验证了状态估计的鲁棒性。

### 3.3.4 实时性测试结果

嵌入式平台上各模块的运行延迟如表5所示：

| 模块                     | 运行延迟（ms） |
|--------------------------|----------------|
| 视觉感知（车道线+障碍物） | 12             |
| LiDAR感知（预处理+聚类） | 10             |
| 多传感器融合             | 5              |
| 端到端总延迟             | 27             |
由表5可知，本文感知系统的端到端总延迟为27ms，小于30ms，满足L2级自动驾驶对感知实时性的要求（通常需≤50ms）。其中，视觉感知模块通过模型轻量化与TensorRT加速，延迟控制在12ms；LiDAR感知模块通过体素下采样降低计算量，延迟为10ms；融合模块因算法优化，延迟仅为5ms。多线程并行处理技术的应用，避免了各模块的串行等待，进一步提升了系统实时性。

## 4 讨论与展望

### 4.1 研究不足

本文提出的无人车多传感器融合感知方案虽在多场景下验证了有效性，但仍存在以下不足：
* 融合算法的适应性有待提升：改进EKF算法的噪声协方差矩阵为固定值，在复杂动态场景（如突发障碍物、剧烈变道）下，难以实时调整噪声参数，可能导致融合精度下降；
* 极端天气下性能有限：在强降雨、暴雪等极端恶劣天气下，LiDAR点云受雨滴、雪花遮挡，视觉图像对比度降低，系统感知精度仍会显著下降；
* 缺乏语义信息融合：当前方案仅融合了障碍物的几何特征与类别信息，未引入交通标识、道路语义等高级语义信息，对复杂交通场景的理解能力不足。

### 4.2 未来展望

* 针对上述不足，未来可从以下方向开展深入研究：
* 自适应融合算法研究：引入自适应卡尔曼滤波、粒子滤波等算法，实时估计噪声协方差矩阵，提升融合算法对动态场景的适应性；
* 极端天气感知优化：结合毫米波雷达与红外相机，利用毫米波雷达不受天气影响、红外相机在低光照下性能优越的优势，构建“视觉+LiDAR+毫米波雷达+红外”的四模态感知框架，提升极端天气下的感知鲁棒性；
* 语义级融合技术：引入Transformer等深度学习架构，实现视觉图像语义分割、LiDAR点云语义分割结果的融合，提升系统对交通场景的语义理解能力；
* 边缘计算与协同感知：利用边缘计算节点的强大算力，实现多无人车的协同感知，通过数据共享提升大范围场景的感知覆盖能力。

## 5 结论

本文针对无人车单传感器感知鲁棒性不足、多源数据融合难、嵌入式平台实时性差等问题，提出一种融合视觉、LiDAR、IMU与GPS的多模态感知框架，主要研究结论如下：
设计了高效的单传感器感知模块：视觉模块采用“传统视觉+深度学习”的混合方案，实现了高精度车道线检测与障碍物识别；LiDAR模块通过统计滤波、RANSAC地面分割与DBSCAN聚类，有效提取了障碍物的三维特征；
提出了改进EKF融合算法：引入视觉/LiDAR特征作为观测值，修正了IMU累积误差与GPS定位偏差，提升了状态估计精度；
搭建了实时嵌入式感知系统：通过模型轻量化、硬件加速与并行计算优化，实现了27ms的端到端感知延迟；
多场景实验验证：在园区、城区、乡村三类场景下的实验表明，系统障碍物检测mAP达94.7%，车道线偏移估计误差小于0.1m，状态估计精度显著优于传统方案，满足L2级自动驾驶的工程需求。
本文提出的感知方案兼顾了精度、鲁棒性与实时性，且采用低成本传感器组合，具有较高的工程应用价值，可为L2-L4级自动驾驶感知系统的设计与实现提供参考。

### 参考文献

* [1] 李克强, 王飞跃, 戴海峰. 智能网联汽车技术发展现状及未来趋势[J]. 中国工程科学, 2020, 22(1): 6-14.
* [2] 陈宝, 李清泉, 杨必胜. 自动驾驶汽车环境感知技术研究进展[J]. 测绘学报, 2021, 50(3): 378-395.
* [3] 刘杰, 张磊, 王树森. 无人车多传感器融合感知技术综述[J]. 汽车工程, 2022, 44(5): 581-592.
* [4] 王硕, 李骏, 张金换. 自动驾驶环境感知传感器配置与融合策略[J]. 机械工程学报, 2020, 56(14): 1-14.
* [5] Zhang H, Wang J, Li Z. Lane detection algorithm based on improved Hough transform and Kalman filter[J]. Journal of Computational Information Systems, 2016, 12(11): 4567-4574.
* [6] 赵祥模, 刘昭度, 马建. 基于背景差分法的车辆检测与跟踪[J]. 中国公路学报, 2015, 28(2): 100-106.
* [7] Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 779-788.
* [8] Ren S, He K, Girshick R, et al. Faster R-CNN: Towards real-time object detection with region proposal networks[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39(6): 1137-1149.
* [9] Li Z, Chen X, Shen S. BEVFormer: Learning bird's-eye-view representation from multi-camera images via spatiotemporal transformers[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 16296-16305.
* [10] Zhou D, Wang J, Lu J, et al. DETR3D: 3D object detection from multi-view images via 3D-to-2D queries[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 15107-15116.
* [11] Fischler M A, Bolles R C. Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography[J]. Communications of the ACM, 1981, 24(6): 381-395.
* [12] Ester M, Kriegel H P, Sander J, et al. A density-based algorithm for discovering clusters in large spatial databases with noise[C]//Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining. 1996: 226-231.
* [13] Qi C R, Su H, Mo K, et al. PointNet: Deep learning on point sets for 3D classification and segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 652-660.
* [14] 杨俊, 张涛, 刘建业. 基于IMU/GPS融合的自动驾驶车辆定位技术[J]. 导航定位与授时, 2021, 8(2): 1-8.
* [15] Gelb A. Applied optimal estimation[M]. Cambridge: MIT Press, 1974.
* [16] Labbé M, Michaud F. RTAB-Map as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation[J]. Journal of Field Robotics, 2014, 31(2): 416-446.
* [17] 王健, 李艳秋, 张宏. 多传感器融合技术在自动驾驶中的应用[J]. 传感器与微系统, 2022, 41(3): 1-4.
* [18] 陈杰, 赵冬斌, 王天然. 视觉与激光雷达数据融合的研究进展[J]. 自动化学报, 2020, 46(8): 1561-1578.
* [19] Li J, Zhang Y, Chen W. Feature-level fusion of camera and lidar for object detection in autonomous driving[J]. IEEE Transactions on Intelligent Transportation Systems, 2022, 23(8): 12072-12082.
* [20] Zhang Y, Wang X, Li J. Decision-level fusion of multi-sensor for object detection in autonomous vehicles[C]//Proceedings of the IEEE International Conference on Intelligent Transportation Systems. 2020: 1-6.
* [21] Chen Y, Mao Y, Zhang J. FusionNet: A deep learning architecture for multi-modal sensor fusion[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2020: 1-9.
* [22] Lang H, Vora S, Caesar H, et al. PointPillars: Fast encoders for object detection from point clouds[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 12697-12705.
* [23] 李博, 王飞跃, 曾大军. 深度学习在多传感器融合中的应用综述[J]. 自动化学报, 2021, 47(7): 1289-1308.
* [24] 英伟达. Jetson Xavier NX技术规格书[EB/OL]. https://developer.nvidia.com/embedded/jetson-xavier-nx, 2022.
* [25] Han S, Mao H, Dally W J. Deep